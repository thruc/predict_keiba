{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bit1ebf9c4dc354488cae1027a09c7ab0e8",
   "display_name": "Python 3.7.4 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 競馬データ取得プログラム(Python)\n",
    "\n",
    "# 1 クローリングとスクレイピングについて\n",
    "\n",
    "\n",
    "## 1.1 クローリングとスクレイピングの定義\n",
    "\n",
    "### クローリング \n",
    "Webページのハイパーリンクをたどって次々にWebページをダウンロードする作業。\n",
    "### スクレイピング \n",
    "ダウンロードしたWebページから必要な情報を抜き出す作業。\n",
    "\n",
    "## 1.2 クローリング・スクレイピングの方法\n",
    "### プログラミング言語  \n",
    "- python,PHP,java,C,Ruby\n",
    "- シェルスクリプト(Unixコマンド)\n",
    "\n",
    "### Webスクレイピングツール  \n",
    "- ExcelやSpreadSheet  \n",
    "- Octoparse   \n",
    "- Scraper  \n",
    "\n",
    "## 1.3 クローリング・スクレイピングでPythonを使うメリット\n",
    "### 言語自体の特性  \n",
    "読みやすく書きやすいPHP、Java、C / C ++と比べ、Pythonは最も簡単  \n",
    "Pythonはプログラムを書いたらすぐに実行することができる対話型  \n",
    "### 強力なライブラリの存在  \n",
    "世界中の開発者が数多くのライブラリを公開しており、簡単に使うことができる  \n",
    "lxmlやBeautifulSoupは有名なスクレイピングライブラリがある。  \n",
    "### スクレイピング後の処理との親和性  \n",
    "データ分析においてもPythonには優秀なライブラリのpandasなどが揃っている\n",
    "Pythonでは数値計算や科学技術計算の分野で古くからNumPyやSciPyといったライブラリが有名　　\n",
    "  \n",
    "## 1.4 今回紹介するライブラリ\n",
    "### Beautiful Soup \n",
    "シンプルかつわかりやすいAPIでデータを抜き出せるのが特徴で、古くから人気のあるライブラリです。  \n",
    "内部のパーサーを目的に応じて切り替えられます。\n",
    "### Selenium   \n",
    "ブラウザーを自動操作するためのライブラリです。  \n",
    "Pythonの他にJavaやJavaScriptなど様々な言語に対応しています。  \n",
    "ブラウザーベンダーがWebDriverAPIを実装するドライバーを用意しており、これを経由して操作します。\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Pythonの基礎知識\n",
    "## 2.1 python実行方法\n",
    "### インタラクティブシェルの使用\n",
    "pythonコマンドを引数なしで実行すると、インタラクティブシェルが起動します。  \n",
    "pythonのコードを対話的に実行できるので、ライブラリの使い方の確認などに便利です。\n",
    "\n",
    "### スクリプトファイルの実行と構成\n",
    "pythonのスクリプトは.pyという拡張子のファイルに保存します。  \n",
    "pythonコマンドにpyファイルのpathを引数として渡すとスクリプトファイルを実行されます。\n",
    "\n",
    "### Jupyter Notebook\n",
    "ノートブックと呼ばれるファイルにプログラムや説明の文章、実行結果などをまとめて管理できるます。  \n",
    "データ分析用のツールです。"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ソースコードを記述し、Shift+Enterで実行できます。\n",
    "print(1+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このように、Jupyter Notebookでは、セルごとにソースコードを記述して、実行を行うことができます。  \n",
    "実行結果はすぐ下に表示され、何度でも再実行できます。  \n",
    "また、Jupyter Notebookでは、ノートブックにグラフを表示することもできます。"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "price = [100, 250, 380, 500, 700]\n",
    "number = [1, 2, 3, 4, 5]\n",
    "\n",
    "# グラフを書く\n",
    "plt.plot(price, number)\n",
    "\n",
    "# グラフのタイトル\n",
    "plt.title(\"price / number\")\n",
    "\n",
    "# x軸のラベル\n",
    "plt.xlabel(\"price\")\n",
    "\n",
    "# y軸のラベル\n",
    "plt.ylabel(\"number\")\n",
    "\n",
    "# 表示する\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ソースコードと説明の文章、実行結果をまとめて保存しておくことができるので  \n",
    "データ分析を試行錯誤しながら行うことがとても簡単に行うことができます。"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 python基本文法"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 データ構造 \n",
    "Pythonでは数値、文字列、リスト、辞書などの基本的なデータ構造を手軽に扱えます。"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数値\n",
    "整数と実数の基本的な四則演算が行えます。"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1\n",
    "type(a) #整数はint型。type()関数でオブジェクトの型を確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 3.14\n",
    "type(b)#実数はfloat型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(14/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(14//3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(14%3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 文字列\n",
    "Unicode文字列を表すstr型とバイト列を表すbytes型があります。  \n",
    "文字列操作は基本的にstr型で行い、ファイルやネットワーク越しのデータの読み書きなど  \n",
    "Python以外との境界でbytes型に変換します。"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type('abc')#文字列はstr型。<class'str'>  文字列は'または\"で囲う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('abc\\n123')#\\nは改行文字"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### リスト \n",
    "\n",
    "複数の値の列をひとまとめに扱うためのデータ型としてリスト(list)があります。  \n",
    "リストは文字列と同じく順序を持ち反復可能な型（シーケンスと呼ばれる）であり  \n",
    "同様の操作をサポートしています。他の言語では配列やArrayと呼ばれるデータ型に対応します。"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "type(l) #[]でリスト（list型）を得る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [1, 2, 3] #値はカンマで区切る。\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [1, 2, 'Three',4.0] #任意のオブジェクトを要素として含められる。\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(l[0])#[n]でn番目の要素を取得する。\n",
    "print(l[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(l[1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(l)) # len()関数でリストの長さを取得する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = l + [5,6]#+演算子でリスト同士を結合したリストを得る。\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.append(7)#append()メソッドで値を末尾に追加する。\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.insert(1,1.5)#insert()メソッドで第1引数のインデックスに第2引数の値を挿入する。\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del l[1]#del文で指定したインデックスの要素を削除する。\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.pop(0)#pop()で指定したインデックスの要素を取得し、リストから削除する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'a,b,c'\n",
    "l = s.split(',')#,で区切りそれぞれリストに入れる\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ','.join(['a','b','c'])#str型のjoin()メソッドでリストを結合した文字列を得る。\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 制御構造と関数・クラス定義\n",
    "\n",
    "Pythonではインデントが大きな意味を持ちます。  \n",
    "Pythonでは読みにくくなることを避けるため  \n",
    "正しくインデントされていないブロックがあるとIndentationErrorというエラーになり実行できません。  \n",
    "インデントを増やす直前の行の末尾には:（コロン）を置きます  \n",
    "制御構造文と関数・クラス定義を書く際に必要になってきます。\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### if文"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 5 # if 文 で 処理 を 分岐 できる。 \n",
    "if a == 1:\n",
    "    print('aは1') # if 文 の 式 が 真 の とき に 実行 さ れる。\n",
    "elif a == 2:\n",
    "    print('aは2') # elif 節 の 式 が 真 の とき に 実行 さ れる（ elif 節 は なく ても 良い）。 \n",
    "else:\n",
    "    print('aは1でも2でもない') # どの 条件 にも 当てはまら なかっ た とき に 実行 さ れる（ else 節 は なく ても 良い"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/Tomo-Horiuchi/predict_keiba/blob/master/image/1.png?raw=true)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for文とwhile文による繰り返し処理"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):#回数を指定した繰り返しには組み込み関数range()を使う。\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#変数iにinの右側のリストの要素が順に代入されて、ブロック内の処理が計7回実行される。\n",
    "l = [1, 2, 'Three', 4.0, 5, 6, 7]\n",
    "for i in l:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "while i<10:#while文で式が真の間繰り返し処理する。\n",
    "    print(i)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 関数定義"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#addという名前の関数を定義する。この関数はaとbの2つの引数を取り、加算した値を返す。 \n",
    "def add(a,b):\n",
    "    return a + b #return文で関数の戻り値を返す。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = add(1,2)#関数の呼び出しは、関数名の後に括弧で引数を指定する。\n",
    "print(a)#3と表示される。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### クラス定義"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Squareという名前のクラスを定義する。 \n",
    "class Square:\n",
    "    #インスタンスが作成された直後に呼び出される特殊なメソッドを定義する(コンストラクタ)\n",
    "    def __init__(self, width, height):\n",
    "        self.width = width # width 属性 に 値 を 格納 する\n",
    "        self.height = height # height 属性 に 値 を 格納 する。\n",
    "    # 面積 を 計算 する メソッド を 定義 する。 \n",
    "    def area(self):\n",
    "        return self.width*self.height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square=Square(100,20)#Rectクラスのインスタンスを作成する。newなどのキーワードは不要。\n",
    "print(type(square))\n",
    "print(square.width) \n",
    "print(square.height)\n",
    "print(square.area())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 組み込み関数  \n",
    "\n",
    "Pythonにはいくつかの組み込み関数が存在し、特に宣言せずに使えます。  \n",
    "これまでに使ったprint()関数やlen()関数も組み込み関数です。  "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 代表的な組み込み関数\n",
    "![](https://github.com/Tomo-Horiuchi/predict_keiba/blob/master/image/2.png?raw=true)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### モジュール  \n",
    "Pythonには豊富な標準ライブラリが付属しています。Pythonのライブラリはモジュールと呼ばれる単位で管理されます  \n",
    "モジュールには複数のクラスや関数が含まれます。  \n",
    "  \n",
    "  \n",
    "![](https://github.com/Tomo-Horiuchi/predict_keiba/blob/master/image/3.png?raw=true)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ライブラリインストール\n",
    "\n",
    "PyPIで公開されているライブラリのインストールにはpipというツールを使用します\n",
    "  \n",
    "pip install ライブラリ 名 でインストールできます\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### インポート文\n",
    "\n",
    "ライブラリをインポートして利用可能にします　　"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "date = datetime.now()\n",
    "print(date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 スクレピング・クローリング実践 "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 スクレピング・クローリング注意事項\n",
    "### 3.1.1 著作権について\n",
    "著作物は著作権法によって保護されます。著作権法第2条では  \n",
    "著作物の要件が「思想又は感情を創作的に表現したものであつて  \n",
    "文芸、学術、美術又は音楽の範囲に属するもの」と定められています。  \n",
    "Webページは基本的に著作物であると考えられるます。  \n",
    "#### スクレピング・クローリングにおいて注意が特に必要な権利\n",
    "- 複製権：収集したWebページを保存する権利\n",
    "- 翻案権：収集したWebページから新たな著作物を創造する権利\n",
    "- 公衆送信権：収集したWebページをサーバーから公開する権利  \n",
    "  　 \n",
    "これらの行為には基本的に著作権者の許諾が必要ですが、私的使用の範囲内の複製など、  \n",
    "使用目的によっては著作権者の許諾なく自由に行うことが認められています。  \n",
    "情報解析を目的とした複製や検索エンジンサービスの提供を目的とした複製・翻案・自動公衆送信が、  \n",
    "著作権者の許諾なく行えるようになっています  \n",
    "\n",
    "#### 利用についての一定の条件\n",
    "- 会員のみが閲覧可能なサイトのクロールには著作権者の許諾が必要なこと\n",
    "- robots.txtやrobotsmetaタグで拒否されているページをクロールしないこと\n",
    "- クロールした後に拒否されたことがわかった場合は保存済みの著作物を消去すること\n",
    "- 検索結果では元のWebページにリンクすること検索結果として表示する著作物は必要と認められる限度内であること\n",
    "- 違法コンテンツであることを知った場合は公衆送信をやめること\n",
    "\n",
    "### 3.1.2 サーバーの負荷\n",
    "クローラーを実行する際には、クロール先のWebサイトの負荷を考慮する必要があります。  \n",
    "あなたのクローラーがWebサーバーの処理能力の多くを占めてしまうと、  \n",
    "他の人がそのWebサイトを閲覧できなくなってしまいます。  \n",
    "商用サイトの場合は、業務妨害となる可能性もあります。\n",
    "#### 同時接続数\n",
    "1つのWebサーバーが同時に処理できる接続数は限られているので、  \n",
    "同時接続数を増やすと、それだけあなたのクローラーがWebサーバーの処理能力を専有してしまいます。  \n",
    "出来るだけ数を少なくし処理が終わったら接続を切るようにする。\n",
    "\n",
    "#### リクエスト間隔\n",
    "間隔を空けずに次々とWebページを取得すると相手のサーバーに負荷をかけます。  \n",
    "常識的なクロールの間隔1秒以上を入れることが望ましいと考えられています。\n",
    "### 3.1.3　robots.txtによる指示\n",
    "robots.txtはWebサイトのトップディレクトリに配置されるテキストファイルです。  \n",
    "例えば、https://www.python.org/ のrobots.txt は https://www.python.org/robots.txt に置かれます。  \n",
    "robots.txtの中身はRobotsExclusionProtocolとして標準化されており、  \n",
    "GoogleやBingなど主要な検索エンジンのクローラーはこの標準に従っていると表明しています。  \n",
    "robots.txtが存在しない場合は、すべてのページのクロールが許可されているとみなします。\n",
    "#### robots.txtのパース  \n",
    "Pythonの標準ライブラリのurllib.robotparserにはrobots.txtをパースするためのRobotFileParserクラスが含まれています。次のようにrobots.txtを簡単に扱えます。\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.robotparser import RobotFileParser\n",
    "\n",
    "rp = RobotFileParser()\n",
    "rp.set_url('https://www.netkeiba.com/robots.txt')#set_url()でrobots.txtのURLを設定する。\n",
    "rp.read()# read() で robots. txt を 読み込む。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rp.crawl_delay('*'))#クロール間隔表す。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数値またはNoneが返される  \n",
    "返された値が数値ならばその秒数分間隔をあけなければならない  \n",
    "今回はNoneであるのでクロール間隔についての指示はない"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#そのURLのクロールが許可されているかどうかを取得できる。\n",
    "print(rp.can_fetch('*',\"https://db.netkeiba.com/horse/2017101835/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True または Falseが返されます  \n",
    "Trueならばクロールが許可されている  \n",
    "Falseならばクロールは許可されていません"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 スクレピング・クローリングでデータを取得"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 RequestsによるWebページの取得\n",
    "Webページを取得するには、ライブラリのRequestsを使います。  \n",
    "HTTPヘッダーの追加やBasic認証など面倒な処理もRequestsには簡単に使えるインターフェイスが用意されています。 "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "import requests#ライブラリをインポートして利用可能にする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://db.netkeiba.com/race/202006030811.html')#get()関数でWebページを取得できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(response))#get()関数の戻り値はResponseオブジェクト。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.status_code)#status_code属性でHTTPステータスコードを取得できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.headers['content-type'])#headers属性でHTTPヘッダーの辞書を取得できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.encoding)#encoding属性でHTTPヘッダーから得られたエンコーディングを取得できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "response.text#text属性でstr型にデコードしたレスポンスボディを取得できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.encoding = response.apparent_encoding #エンコードを変更"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 BeautifulSoupによるスクレイピング\n",
    "#### 基本文法"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup #bs4モジュールからBeautifulSoupクラスをインポートする。 \n",
    "soup = BeautifulSoup(response.text, 'html.parser')# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(\"a\")#タグ名の属性でh1要素を取得できる。一番先頭のタグのみ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(soup.find(\"a\").get_text))#要素はTagオブジェクト。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(\"a\")#h1タグのすべての要素を取得できます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(soup.find_all(\"a\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = soup.find_all(\"a\")\n",
    "#list型と同様に扱えます\n",
    "print(len(rs))\n",
    "print(rs[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(\"a\").name#Tagオブジェクトのname属性でタグ名を取得できる。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(\"a\").contents#contents属性で子要素（NavigableStringを含む）のリストを取得できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(\"div\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.div.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(soup.h1.text)#text属性で得られる文字列はstrオブジェクト。<class'str'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(\"div\", class_=\"data_intro\")#キーワード引数でclassなどの属性を指定できる。classは予約語なのでclass_を使うこと"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(\"div\", class_=\"data_intro\").get_text()#text属性で要素内のすべての文字列を結合した文字列を取得できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(soup.find(\"div\", class_=\"data_intro\").get_text()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 目的の値をスクレイピングするまでの流れ  \n",
    "  \n",
    "  \n",
    "![](https://github.com/Tomo-Horiuchi/predict_keiba/blob/master/image/4.png?raw=true)\n",
    "![](https://github.com/Tomo-Horiuchi/predict_keiba/blob/master/image/5.png?raw=true)\n",
    "![](https://github.com/Tomo-Horiuchi/predict_keiba/blob/master/image/6.png?raw=true)\n",
    "![](https://github.com/Tomo-Horiuchi/predict_keiba/blob/master/image/7.png?raw=true)\n",
    "![](https://github.com/Tomo-Horiuchi/predict_keiba/blob/master/image/8.png?raw=true)\n",
    "![](https://github.com/Tomo-Horiuchi/predict_keiba/blob/master/image/9.png?raw=true)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目的の数値はclass属性が\"race_table_01 nk_tb_common\"のtableタグで囲まれた二番目のtrタグのにあります  \n",
    "更にそのtrタグのに囲まれた１番目のaタグのにあります"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://db.netkeiba.com/race/202006030811.html')\n",
    "response.encoding = response.apparent_encoding \n",
    "html = response.text\n",
    "soup = BeautifulSoup(html, 'html.parser')# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_table = soup.find(\"table\", class_=\"race_table_01 nk_tb_common\")#class属性が\"race_table_01 nk_tb_common\"のtableタグ取得\n",
    "soup_table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_tr = soup_table.find_all('tr')[1]#二番目のtrタグ取得\n",
    "soup_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_a = soup_tr.find(\"a\")#１番目のaタグを取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_a.get_text()#タグの中のテキストを取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(\"table\", class_=\"race_table_01 nk_tb_common\").find_all(\"tr\")[1].find(\"a\").get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Seleniumによるスクレイピング\n",
    "Seleniumはブラウザーを自動操作するためのライブラリです。  \n",
    "Pythonの他にJavaやJavaScriptなど様々な言語に対応しています。  \n",
    "元々はWebアプリケーションの自動テストツールとして発展しました。ブラウザーベンダーがWebDriverAPIを実装するドライバーを用意しており、これを経由して操作します。  \n",
    "  \n",
    "\n",
    "![](https://github.com/Tomo-Horiuchi/predict_keiba/blob/master/image/10.png?raw=true)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "\n",
    "race_data_columns=[\n",
    "    'race_id',\n",
    "    'race_round',\n",
    "    'race_title',\n",
    "    'race_course',\n",
    "    'weather',\n",
    "    'ground_status',\n",
    "    'time',\n",
    "    'date',\n",
    "    'where_racecourse',\n",
    "    'total_horse_number',\n",
    "    'frame_number_first',\n",
    "    'horse_number_first',\n",
    "    'frame_number_second',\n",
    "    'horse_number_second',\n",
    "    'frame_number_third',\n",
    "    'horse_number_third',\n",
    "    'tansyo',\n",
    "    'hukusyo_first',\n",
    "    'hukusyo_second',\n",
    "    'hukusyo_third',\n",
    "    'wakuren',\n",
    "    'umaren',\n",
    "    'wide_1_2',\n",
    "    'wide_1_3',\n",
    "    'wide_2_3',\n",
    "    'umatan',\n",
    "    'renhuku3',\n",
    "    'rentan3'\n",
    "    ]\n",
    "\n",
    "horse_data_columns=[\n",
    "    'race_id',\n",
    "    'rank',\n",
    "    'frame_number',\n",
    "    'horse_number',\n",
    "    'horse_id',\n",
    "    'sex_and_age',\n",
    "    'burden_weight',\n",
    "    'rider_id',\n",
    "    'goal_time',\n",
    "    'goal_time_dif',\n",
    "    'time_value',\n",
    "    'half_way_rank',\n",
    "    'last_time',\n",
    "    'odds',\n",
    "    'popular',\n",
    "    'horse_weight',\n",
    "    'tame_time',\n",
    "    'tamer_id',\n",
    "    'owner_id'\n",
    "]\n",
    "\n",
    "\n",
    "def scrape_race_and_horse_data_by_html(race_id, html):\n",
    "    \"\"\"\n",
    "    HTMLファイルからレースと馬のデータを取得\n",
    "    戻り値1: list型 レースデータ\n",
    "    戻り値2: list型 馬データ\n",
    "    \"\"\"\n",
    "    race_list = [race_id]\n",
    "    horse_list_list = []\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    #####################\n",
    "    # race_data\n",
    "    #####################\n",
    "    data_intro = soup.find(\"div\", class_=\"data_intro\")\n",
    "    \n",
    "    race_list.append(data_intro.find(\"dt\").get_text().strip(\"\\n\")) # race_round\n",
    "    race_list.append(data_intro.find(\"h1\").get_text().strip(\"\\n\")) # race_title\n",
    "\n",
    "    race_details1 = data_intro.find(\"p\").get_text().strip(\"\\n\").split(\"\\xa0/\\xa0\")\n",
    "    race_list.append(race_details1[0]) # race_course\n",
    "    race_list.append(race_details1[1]) # weather\n",
    "    race_list.append(race_details1[2]) # ground_status\n",
    "    race_list.append(race_details1[3][5:10]) # time\n",
    "\n",
    "    race_details2 = data_intro.find(\"p\", class_=\"smalltxt\").get_text().strip(\"\\n\").split(\" \")\n",
    "    race_list.append(race_details2[0]) # date\n",
    "    race_list.append(race_details2[1]) # where_racecourse\n",
    "\n",
    "\n",
    "    result_rows = soup.find(\"table\", class_=\"race_table_01 nk_tb_common\").findAll('tr') \n",
    "    race_list.append(len(result_rows)-1) # total_horse_number\n",
    "    for i in range(1,4):\n",
    "        row = result_rows[i].findAll('td')\n",
    "        race_list.append(row[1].get_text()) # frame_number_first or second or third\n",
    "        race_list.append(row[2].get_text()) # horse_number_first or second or third\n",
    "\n",
    "\n",
    "    pay_back_tables = soup.findAll(\"table\", class_=\"pay_table_01\")\n",
    "\n",
    "    pay_back1 = pay_back_tables[0].findAll('tr')\n",
    "    race_list.append(pay_back1[0].find(\"td\", class_=\"txt_r\").get_text()) #tansyo\n",
    "    hukuren = pay_back1[1].find(\"td\", class_=\"txt_r\")\n",
    "    tmp = []\n",
    "    for string in hukuren.strings:\n",
    "        tmp.append(string)\n",
    "    for i in range(3):\n",
    "        try:\n",
    "            race_list.append(tmp[i]) # hukuren_first or second or third\n",
    "        except IndexError:\n",
    "            race_list.append(\"0\")\n",
    "    try:\n",
    "        race_list.append(pay_back1[2].find(\"td\", class_=\"txt_r\").get_text())\n",
    "    except IndexError:\n",
    "        race_list.append(\"0\")\n",
    "\n",
    "    try:\n",
    "        race_list.append(pay_back1[3].find(\"td\", class_=\"txt_r\").get_text())\n",
    "    except IndexError:\n",
    "        race_list.append(\"0\")\n",
    "\n",
    "\n",
    "\n",
    "    pay_back2 = pay_back_tables[1].findAll('tr')\n",
    "    # wide 1&2\n",
    "    wide = pay_back2[0].find(\"td\", class_=\"txt_r\")\n",
    "    tmp = []\n",
    "    for string in wide.strings:\n",
    "        tmp.append(string)\n",
    "    for i in range(3):\n",
    "        try:\n",
    "            race_list.append(tmp[i]) # hukuren_first or second or third\n",
    "        except IndexError:\n",
    "            race_list.append(\"0\")\n",
    "\n",
    "    # umatan\n",
    "    race_list.append(pay_back2[1].find(\"td\", class_=\"txt_r\").get_text()) #umatan\n",
    "\n",
    "    race_list.append(pay_back2[2].find(\"td\", class_=\"txt_r\").get_text()) #renhuku3\n",
    "    try:\n",
    "        race_list.append(pay_back2[3].find(\"td\", class_=\"txt_r\").get_text()) #rentan3\n",
    "    except IndexError:\n",
    "        race_list.append(\"0\")\n",
    "    ####################\n",
    "    # horse data\n",
    "    ####################\n",
    "    for rank in range(1, len(result_rows)):\n",
    "        horse_list = [race_id]\n",
    "        result_row = result_rows[rank].findAll(\"td\")\n",
    "        # rank\n",
    "        horse_list.append(result_row[0].get_text().strip())\n",
    "        # frame_number\n",
    "        horse_list.append(result_row[1].get_text().strip())\n",
    "        # horse_number\n",
    "        horse_list.append(result_row[2].get_text().strip())\n",
    "        # horse_id\n",
    "        horse_list.append(result_row[3].find('a').get('href').split(\"/\")[-2].strip())\n",
    "        # sex_and_age\n",
    "        horse_list.append(result_row[4].get_text().strip())\n",
    "        # burden_weight\n",
    "        horse_list.append(result_row[5].get_text().strip())\n",
    "        # rider_id\n",
    "        horse_list.append(result_row[6].find('a').get('href').split(\"/\")[-2].strip())\n",
    "        # goal_time\n",
    "        horse_list.append(result_row[7].get_text().strip())\n",
    "        # goal_time_dif\n",
    "        horse_list.append(result_row[8].get_text().strip())\n",
    "        # time_value(premium)\n",
    "        horse_list.append(result_row[9].get_text().strip())\n",
    "        # half_way_rank\n",
    "        horse_list.append(result_row[10].get_text().strip())\n",
    "        # last_time\n",
    "        horse_list.append(result_row[11].get_text().strip())\n",
    "        # odds\n",
    "        horse_list.append(result_row[12].get_text().strip())\n",
    "        # popular\n",
    "        horse_list.append(result_row[13].get_text().strip())\n",
    "        # horse_weight\n",
    "        horse_list.append(result_row[14].get_text().strip())\n",
    "        # tame_time(premium)\n",
    "        horse_list.append(result_row[15].get_text().strip())\n",
    "        # tamer_id\n",
    "        horse_list.append(result_row[18].find('a').get('href').split(\"/\")[-2].strip())\n",
    "        # owner_id\n",
    "        horse_list.append(result_row[19].find('a').get('href').split(\"/\")[-2].strip())\n",
    "\n",
    "        horse_list_list.append(horse_list)\n",
    "\n",
    "    return race_list, horse_list_list\n",
    "\n",
    "def request_html(url):\n",
    "    \"\"\"\n",
    "    URLからHTMLファイルを取得する\n",
    "    戻り値: str型　HTMLのデータ\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    response.encoding = response.apparent_encoding\n",
    "    html = response.text\n",
    "    time.sleep(1)\n",
    "    return html\n",
    "\n",
    "def create_csv(urls):\n",
    "    \"\"\"\n",
    "    URLからレースと馬のデータを取得しCSVに保存する\n",
    "    戻り値1:list型 レース\n",
    "    戻り値2:list型　馬のデータ\n",
    "    \"\"\"\n",
    "    CSV_DIR = \"csv\"\n",
    "    if not os.path.isdir(CSV_DIR):\n",
    "        os.makedirs(CSV_DIR)\n",
    "    save_race_csv = CSV_DIR+\"/race.csv\"\n",
    "    horse_race_csv = CSV_DIR+\"/horse.csv\"\n",
    "    race_df = pd.DataFrame(columns=race_data_columns )\n",
    "    horse_df = pd.DataFrame(columns=horse_data_columns )\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for url in urls:\n",
    "        print(url)\n",
    "        list = url.split(\"/\")\n",
    "        race_id = list[-2]\n",
    "        \n",
    "        html = request_html(url)\n",
    "        race_list, horse_list_list = scrape_race_and_horse_data_by_html(race_id, html)\n",
    "        #horse_data\n",
    "        for horse_list in horse_list_list:\n",
    "            horse_se = pd.Series( horse_list, index=horse_df.columns)\n",
    "            horse_df = horse_df.append(horse_se, ignore_index=True)\n",
    "        #race_data\n",
    "        race_se = pd.Series(race_list, index=race_df.columns )\n",
    "        race_df = race_df.append(race_se, ignore_index=True )\n",
    "\n",
    "        #250ごとにデータをCSVに変換しておく\n",
    "        if count == 250:\n",
    "            count = 0\n",
    "            race_df.to_csv(save_race_csv, header=True, index=False)\n",
    "            horse_df.to_csv(horse_race_csv, header=True, index=False)\n",
    "\n",
    "        \n",
    "\n",
    "    race_df.to_csv(save_race_csv, header=True, index=False)\n",
    "    horse_df.to_csv(horse_race_csv, header=True, index=False)\n",
    "    return race_list, horse_list_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\"https://db.netkeiba.com/race/202006030811.html\"]\n",
    "\n",
    "race_list, horse_list_list = create_csv(urls)\n",
    "\n",
    "print(\"------------------race_list-------------------\")\n",
    "print(race_list)\n",
    "print(\"------------------horse_list_list-------------------\")\n",
    "print(horse_list_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下のセルはColaboratory上でseleniumを動かす場合のみ実行"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update\n",
    "!apt install chromium-chromedriver\n",
    "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import Select,WebDriverWait\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium import webdriver\n",
    "import chromedriver_binary\n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "import time\n",
    "\n",
    "def get_urls(year, mon):\n",
    "    urls = []\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    ###\n",
    "    #Colaboratory上でseleniumを動かす場合のみ実行\n",
    "    #options = webdriver.ChromeOptions()\n",
    "    #options.add_argument('--headless')\n",
    "    #options.add_argument('--no-sandbox')\n",
    "    #options.add_argument('--disable-dev-shm-usage')\n",
    "    #driver = webdriver.Chrome('chromedriver',options=options)\n",
    "    ####\n",
    "    driver = webdriver.Chrome() #Colaboratory上でseleniumを動かす場合この行をコメントアウト\n",
    "    wait = WebDriverWait(driver,10)\n",
    "    URL = \"https://db.netkeiba.com/?pid=race_search_detail\"\n",
    "    driver.get(URL)\n",
    "    time.sleep(1)\n",
    "    wait.until(EC.presence_of_all_elements_located)\n",
    "\n",
    "\n",
    "    start_year = year\n",
    "    start_mon = mon\n",
    "    end_year = year\n",
    "    end_mon = mon\n",
    "\n",
    "    start_year_element = driver.find_element_by_name('start_year')\n",
    "    start_year_select = Select(start_year_element)\n",
    "    start_year_select.select_by_value(str(start_year))\n",
    "    start_mon_element = driver.find_element_by_name('start_mon')\n",
    "    start_mon_select = Select(start_mon_element)\n",
    "    start_mon_select.select_by_value(str(start_mon))\n",
    "    end_year_element = driver.find_element_by_name('end_year')\n",
    "    end_year_select = Select(end_year_element)\n",
    "    end_year_select.select_by_value(str(end_year))\n",
    "    end_mon_element = driver.find_element_by_name('end_mon')\n",
    "    end_mon_select = Select(end_mon_element)\n",
    "    end_mon_select.select_by_value(str(end_mon))\n",
    "\n",
    "    for i in range(1,11):\n",
    "        terms = driver.find_element_by_id(\"check_Jyo_\"+ str(i).zfill(2))\n",
    "        terms.click()\n",
    "    \n",
    "    list_element = driver.find_element_by_name('list')\n",
    "    list_select = Select(list_element)\n",
    "    list_select.select_by_value(\"100\")\n",
    "\n",
    "    frm = driver.find_element_by_css_selector(\"#db_search_detail_form > form\")\n",
    "    frm.submit()\n",
    "    time.sleep(10)\n",
    "    wait.until(EC.presence_of_all_elements_located)\n",
    "    while True:\n",
    "        time.sleep(10)\n",
    "        wait.until(EC.presence_of_all_elements_located)\n",
    "        all_rows = driver.find_element_by_class_name('race_table_01').find_elements_by_tag_name(\"tr\")\n",
    "        for row in range(1, len(all_rows)):\n",
    "            race_href=all_rows[row].find_elements_by_tag_name(\"td\")[4].find_element_by_tag_name(\"a\").get_attribute(\"href\")\n",
    "            urls.append(race_href)\n",
    "        try:\n",
    "            target = driver.find_elements_by_link_text(\"æ¬¡\")[0]\n",
    "            time.sleep(4)\n",
    "            driver.execute_script(\"arguments[0].click();\", target)\n",
    "        except IndexError:\n",
    "            break\n",
    "    return urls\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2020\n",
    "mon = 1\n",
    "urls = get_urls(year, mon)\n",
    "create_csv(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}